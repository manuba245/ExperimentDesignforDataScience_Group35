{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning library\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Input, Flatten, GRU, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "\n",
    "# set seeds for reproducability\n",
    "from numpy.random import seed\n",
    "seed(4222)\n",
    "\n",
    "# general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os, io\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aolQueries = []\n",
    "\n",
    "for i in range(2, 10):\n",
    "  if i < 10:\n",
    "    fileName = \"user-ct-test-collection-0\" + str(i) + \".txt\"\n",
    "  else:\n",
    "    fileName = \"user-ct-test-collection-\" + str(i) + \".txt\"\n",
    "\n",
    "  lines = []\n",
    "  with open(fileName) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "  count = 0\n",
    "  for line in lines:\n",
    "    if count > 0:\n",
    "      query = line.split(\"\\t\")[1]\n",
    "      aolQueries.append(query)\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = aolQueries[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1201, 3421],\n",
       " [4801, 4802],\n",
       " [4801, 4802, 4803],\n",
       " [1201, 3421],\n",
       " [1201, 3421, 234],\n",
       " [1201, 3421, 234, 4804],\n",
       " [257, 3422],\n",
       " [257, 3422, 1569],\n",
       " [257, 3422, 1569, 3],\n",
       " [257, 3422, 1569, 3, 350]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding - convert from text to sequences (numbers)\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    '''\n",
    "    Convert each sentence to a list of ngram sequences\n",
    "    '''\n",
    "    # Tokenization\n",
    "    tokenizer.fit_on_texts(corpus) # Fit on our text sentences\n",
    "    total_words = len(tokenizer.word_index) + 1 # Total number of unique words in our vocabulary\n",
    "    \n",
    "    # Convert data to sequence of tokens \n",
    "    input_sequences = [] # House our final sequences\n",
    "    for line in corpus: # For every sentence\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0] # Convert a line of text to a line of sequence\n",
    "        for i in range(1, len(token_list)): # Generate ngrams\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10] # The first 10 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    '''\n",
    "    Pad every sentence to the longest sentence in the corpus\n",
    "    '''\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = np_utils.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sentence length is: 43\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0 1201]\n"
     ]
    }
   ],
   "source": [
    "print(\"The maximum sentence length is:\", max_sequence_len)\n",
    "print(predictors[0]) # Padded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 42, 100)           826900    \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 200)              121200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8269)              1662069   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,610,169\n",
      "Trainable params: 2,610,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "\n",
    "    # Initialise model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_sequence_len - 1))\n",
    "    \n",
    "    model.add(Bidirectional(GRU(100)))\n",
    "    \n",
    "    # Output Layer - softmax activation\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    # Compile model - crossentropy loss\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "1004/1004 [==============================] - 84s 75ms/step - loss: 7.1574\n",
      "Epoch 2/11\n",
      "1004/1004 [==============================] - 75s 75ms/step - loss: 5.3555\n",
      "Epoch 3/11\n",
      "1004/1004 [==============================] - 76s 76ms/step - loss: 4.0442\n",
      "Epoch 4/11\n",
      "1004/1004 [==============================] - 75s 75ms/step - loss: 3.0779\n",
      "Epoch 5/11\n",
      "1004/1004 [==============================] - 75s 75ms/step - loss: 2.3915\n",
      "Epoch 6/11\n",
      "1004/1004 [==============================] - 80s 79ms/step - loss: 1.9197\n",
      "Epoch 7/11\n",
      "1004/1004 [==============================] - 78s 78ms/step - loss: 1.5977\n",
      "Epoch 8/11\n",
      "1004/1004 [==============================] - 77s 77ms/step - loss: 1.3671\n",
      "Epoch 9/11\n",
      "1004/1004 [==============================] - 77s 76ms/step - loss: 1.2068\n",
      "Epoch 10/11\n",
      "1004/1004 [==============================] - 79s 79ms/step - loss: 1.0954\n",
      "Epoch 11/11\n",
      "1004/1004 [==============================] - 77s 76ms/step - loss: 1.0136\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, label, epochs=11, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The crossentropy loss is: 1.0136362314224243\n"
     ]
    }
   ],
   "source": [
    "loss_history = history.history[\"loss\"][-1]\n",
    "print(\"The crossentropy loss is:\", loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] # Tokenize seed text\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # Pad seed text\n",
    "        predicted = model.predict(token_list, verbose=0) # Predict next word given seeded text\n",
    "        \n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word,index in tokenizer.word_index.items(): # Convert from sequence to string\n",
    "            if  index == predicted[0,_]:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"getting organized at work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Getting Organized At Work '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(search_query,1,max_sequence_len)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5322818cae43cf6468214953b049884cb6ca7e8071f01fca2a6d6ce521d192bd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
